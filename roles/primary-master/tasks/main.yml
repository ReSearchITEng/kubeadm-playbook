---
- set_fact:
    env_kc: '{{ proxy_env |default({}) | combine ({"KUBECONFIG" :"/etc/kubernetes/admin.conf"}) }}'
  tags:
  - always

- name: decide_master_name for join
  include_role:
    name: common
    tasks_from: decide_master_name

#- name: Install packages required by rook (ceph) storage setup
#  package: name={{ item }} state={{ package_state | default ('present') }}
#  when: rook is defined and rook.enabled
#  with_items:
#  - jq
#  - ceph-common

## PULL IMAGES
#- name: Pull master images on master - when DOCKER_IMAGES is defined
#  #TODO: to be removed/replaced in 1.11 with the kubeadm pull command (kubeadm config images pull --kubernetes-version v11.0.3 ), but better remove, as kubeadm pre-flight does the pull now anyway...
#  #command: /usr/bin/docker pull "{{ item }}:{{ ClusterConfiguration.kubernetesVersion | default ('latest') }}"
#  #command: /usr/bin/docker pull "{{ images_repo |default ('k8s.gcr.io') }}/{{ item }}:{{ ClusterConfiguration.kubernetesVersion | default ('latest') }}"
#  command: /usr/bin/docker pull "{{ item.name }}:{{ item.tag }}"
#  with_items: "{{ DOCKER_IMAGES }}"
#  tags:
#  - prepull_images
#  register: command_result
#  changed_when: '"Image is up to date" not in command_result.stdout or "Already exists" not in command_result.stdout'
#  #when: pre_pull_k8s_images is defined and pre_pull_k8s_images
#  #when: pre_pull_k8s_images is defined and pre_pull_k8s_images and ClusterConfiguration.kubernetesVersion is defined
#  #when: full_kube_reinstall is defined and full_kube_reinstall and ClusterConfiguration.kubernetesVersion is defined
#  when: 
#  - DOCKER_IMAGES is defined
#  - pre_pull_k8s_images is defined 
#  - pre_pull_k8s_images 
#  - images_repo is defined
#  #- ClusterConfiguration.kubernetesVersion is defined
#  #- ClusterConfiguration.apiVersion == "kubeadm.k8s.io/v1alpha1"
#  #when: full_kube_reinstall is defined and full_kube_reinstall and ClusterConfiguration.kubernetesVersion is defined

- name: Pull images on master (if defined, it uses images_repo; otherwise, defaults to k8s.gcr.io )
  command: "kubeadm config images pull --kubernetes-version {{ ClusterConfiguration.kubernetesVersion }} --image-repository {{ images_repo |default ('k8s.gcr.io') }}"
  tags:
  - prepull_images
  register: command_result
  changed_when: '"Image is up to date" not in command_result.stdout or "Already exists" not in command_result.stdout'
  when: 
  - pre_pull_k8s_images is defined 
  - pre_pull_k8s_images 
  #- DOCKER_IMAGES is not defined ## or images_repo is not defined
  - ClusterConfiguration.kubernetesVersion is version_compare ( 'v1.11', '>=' )

- name: making sure there is an apiServer section under ClusterConfiguration
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'apiServer' : {} }, recursive=True) }}"
  when: ClusterConfiguration is defined

- name: making sure there is a controllerManager section under ClusterConfiguration
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'controllerManager' : {} }, recursive=True) }}"
  when: ClusterConfiguration is defined

## SANs
- name: adding {{ inventory_hostname,inventory_hostname_short }} to the ClusterConfiguration.apiServer.certSANs
  set_fact:
    extended_cert_list: "{{ [ansible_default_ipv4.address,inventory_hostname,inventory_hostname_short,custom.networking.masterha_fqdn,'127.0.0.1',custom.networking.masterha_ip | default(ansible_fqdn)] | union (ClusterConfiguration.apiServer.certSANs | default([]) ) }}"
    #extended_cert_list: "{{ [inventory_hostname,inventory_hostname_short] | union (ClusterConfiguration.apiServer.certSANs | default([kubernetes]) ) }}"
  when: ClusterConfiguration is defined 

- name: merging {{ inventory_hostname,inventory_hostname_short }} to the ClusterConfiguration.apiServer.certSANs
  set_fact:
    ClusterConfiguration: "{{ ClusterConfiguration | combine ({ 'apiServer' : {'certSANs' : extended_cert_list } }, recursive=True) }}"
  when: extended_cert_list is defined and ClusterConfiguration is defined

## Cloud-Config - pre cluster init steps
  ## Add the cloud-config to the ClusterConfiguration for kubeadm.k8s.io/v1alpha2 and above
  ## Note: ClusterConfiguration is used only on the primary-master. The secondary-masters take it dynamically.
  ## as the ClusterConfiguration.cloudProvider has been deprecated. We use ClusterConfiguration.cloudProvider as conditional for triggering below
  ## as well as triggering updates to the /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (in some other step)
- block:
  - name: add cloud-config to apiServer.extraVolumes
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'apiServer' : {'extraVolumes': ClusterConfiguration.apiServer.extraVolumes | default ([]) | union ([{'name': 'cloud', 'hostPath': '/etc/kubernetes/cloud-config',  'mountPath': '/etc/kubernetes/cloud-config' }] ) } }, recursive=True ) }}"

  - name: add cloud-config to apiServer.extraArgs
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'apiServer' : {'extraArgs': ClusterConfiguration.apiServer.extraArgs | default ({}) | combine ( {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' }, recursive=True ) } }, recursive=True ) }}"

  - name: add cloud-config to controllerManager.extraVolumes
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'controllerManager' : {'extraVolumes': ClusterConfiguration.controllerManager.extraVolumes | default ([]) | union ([{'name': 'cloud', 'hostPath': '/etc/kubernetes/cloud-config',  'mountPath': '/etc/kubernetes/cloud-config' }] ) } }, recursive=True ) }}"

  - name: add cloud-config to controllerManager.extraArgs
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( { 'controllerManager' : {'extraArgs': ClusterConfiguration.controllerManager.extraArgs | default ({}) | combine ( {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' } , recursive=True ) } }, recursive=True ) }}"

  - name: InitConfiguration - cloudProvider merging {{ ClusterConfiguration.cloudProvider }} to the InitConfiguration.nodeRegistration.kubeletExtraArgs
    set_fact:
      InitConfiguration: "{{  InitConfiguration | combine ( { 'nodeRegistration': {'kubeletExtraArgs': {'cloud-provider': ClusterConfiguration.cloudProvider, 'cloud-config': '/etc/kubernetes/cloud-config' }  } }, recursive=True) }}"
  when:
  - ClusterConfiguration is defined
  - ClusterConfiguration.cloudProvider is defined
  - ( ClusterConfiguration.apiVersion >= "kubeadm.k8s.io/v1alpha3" ) or (ClusterConfiguration.apiVersion == "kubeadm.k8s.io/v1")

### MasterHA: ETCD Setup on master
# section removed starting v1.14

### MasterHA : point api.controlPlaneEndpoint and api.advertiseAddress to {{ custom.networking.masterha_ip }}
- block:
  - name: MasterHA merging {{ custom.networking.masterha_ip }} to the ClusterConfiguration.controlPlaneEndpoint
    set_fact:
      ClusterConfiguration: "{{  ClusterConfiguration | combine ( {'controlPlaneEndpoint': custom.networking.masterha_ip + ':' + custom.networking.masterha_bindPort | default (6443) | string }, recursive=True) }}"

  ## TODO: try to remove this, and keep the autodetermined addr:
  # - name: MasterHA merging {{ ansible_default_ipv4.address }} to the InitConfiguration.localAPIEndpoint.advertiseAddress ; kubeadm determines addr auto (and no DNS; ip is mandatory);
  #   set_fact:
  #     InitConfiguration: "{{  InitConfiguration | combine ( { 'localAPIEndpoint': {'advertiseAddress': ansible_default_ipv4.address } }, recursive=True) }}"
  when: 
  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']

### Configuration is prepared, show it, write it, use it
- name: "debug: This is the master init configuration to be used (verbosity 2 or above):"
  debug: var={{item}} verbosity=2
  changed_when: false
  with_items:
  - ClusterConfiguration
  - InitConfiguration
  - KubeProxyConfiguration
  - KubeletConfiguration

- name: Make sure /etc/kubernetes folder exists
  file: path=/etc/kubernetes/ state=directory mode=0755

- name: Writing ClusterConfiguration and InitConfiguration to /etc/kubernetes/kubeadm-master.conf
  copy:
    dest: /etc/kubernetes/kubeadm-master.conf
    force: yes
    content: |
      {{ ClusterConfiguration | to_nice_yaml }}
      ---
      {{ InitConfiguration | to_nice_yaml }}
      ---
      {{ KubeProxyConfiguration | to_nice_yaml }}
      ---
      {{ KubeletConfiguration | to_nice_yaml }}
  when:
  - inventory_hostname in groups['primary-master']

- name: "Initialize cluster on primary master with kubeadm init {{kubeadm_init_args}} --config /etc/kubernetes/kubeadm-master.conf --upload-certs"
  # Note: one cannot merge config from both config file anc cli. Only the config file will be used (when present)
#  environment: '{{env_kc}}'
  command: /usr/bin/kubeadm init {{ kubeadm_init_args | default(" ") }} --config /etc/kubernetes/kubeadm-master.conf --upload-certs
  register: kubeadm_init_primary
  tags:
  - init
  when:
  # - groups['masters'] | length > 1 # Allow this for both HA primary and non-HA (exclude ha secondary masters)
  - inventory_hostname in groups['primary-master']

- name: kubeadm_init_primary output
  debug: msg:"{{kubeadm_init_primary.stdout_lines}}"
  when:
  - inventory_hostname in groups['primary-master']

- name: kubeadm_init_primary output var
  debug: var=kubeadm_init_primary verbosity=3
  when:
  - inventory_hostname in groups['primary-master']

- name: "Wait 500 seconds for primary-master to respond: {{ InitConfiguration.localAPIEndpoint.advertiseAddress | default (master_name) }}:{{ InitConfiguration.localAPIEndpoint.bindPort | default (6443) }} "
  #master_name
  wait_for:
    port: "{{ InitConfiguration.localAPIEndpoint.bindPort | default (6443) }}"
    host: "{{ InitConfiguration.localAPIEndpoint.advertiseAddress | default (master_name) }}" #master_name
    delay: 1
    timeout: 500
  run_once: yes
  tags:
  - init
  when:
#  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']

### TODO: remake it in 1.15 !!!
  # - name: generate a join token on primary-master # TEMPORARY 1.14 till this is fixed: https://github.com/kubernetes/kubeadm/issues/1485
  #   shell: 'kubeadm token create --description "kubeadm-playbook-node-joining-token" --ttl 15m --print-join-command '
  #   # | awk '{print $5}{print $7}'
  #   environment: '{{env_kc}}'
  #   # environment -> is required due to a k8s bug which makes kubeadm need internet to generate a token. setting version is not allowed
  #   # Optionally using "--config /etc/kubernetes/kubeadm-master.conf" to get rid of the message that it tries to connect to internet for version
  #   register: kubeadm_token_whash_secondarymasters
  #   delegate_to: "{{groups['primary-master'][0]}}"
  #   run_once: yes

### TODO: remake it in 1.15 !!!
#### WARNING: 1.14 TEMPORARY REMOVE FROM SECONDARY due to kubeamd issue 1485: --config /etc/kubernetes/kubeadm-master.conf 
  # - #name: "Join cluster with {{kubeadm_init_args}}  --experimental-control-plane --certificate-key {{ kubeadm_upload_certificate_key.stdout_lines[0] }}"
  #   name: '{{ kubeadm_token_whash_secondarymasters.stdout }} {{ kubeadm_init_args | default(" ") }} --experimental-control-plane --certificate-key ...'
  #   command: '{{kubeadm_token_whash_secondarymasters.stdout}} {{ kubeadm_init_args | default(" ") }} --experimental-control-plane --certificate-key {{ kubeadm_upload_certificate_key.stdout_lines[0] }}'
  #   #/usr/bin/kubeadm join
  #   register: kubeadm_join_secondary

  # - name: kubeadm_join_secondary output
  #   debug: msg:"{{kubeadm_join_secondary.stdout_lines}}"

  # - name: kubeadm_join_secondary output var
  #   debug: var=kubeadm_join_secondary verbosity=3

  # when:
  # - groups['masters'] | length > 1 
  # - inventory_hostname not in groups['primary-master']
  # tags:
  # - init
  # - init_secondary_masters

#This is mandatory especially when proxy is used, and the inventory_hostname is defined with fqdn
#important, in order to ensure the connection to local server is not going via proxy (expecially when applying addons)
#we are going to skip this when proxy_env is not defined due to issues like: https://github.com/ReSearchITEng/kubeadm-playbook/issues/81
#where ansible_fqdn in decide_master_name.yml is different from inventory_hostname
- name: update in admin.conf and kubelet.conf the name of primary-master detected {{ master_name }}
  replace:
    dest: '{{ item }}'
    regexp: '(\s+)(server: https:\/\/)[A-Za-z0-9\-\.]+:'
    replace: '\1\2{{ master_name }}:'
    backup: yes
  when: 
  - proxy_env is defined 
  - proxy_env.http_proxy is defined or proxy_env.https_proxy is defined
  #- and test master is defined with fqdn in the inventory file (e.g. master.example.com)
  with_items:
  - /etc/kubernetes/admin.conf
  - /etc/kubernetes/kubelet.conf
  #- /etc/kubernetes/controller-manager.conf
  #- /etc/kubernetes/scheduler.conf
  tags:
  - init
  - fqdn
  - proxy
  notify:
  - Restart kubelet

# master node delete and kubelet restart is needed for 1.14+, for cloud=vsphere, otherwise we get: "Unable to find VM by UUID. VM UUID:" or Error "No VM found" node info for node
- name: "when ClusterConfiguration.cloudProvider "
  block:

  - name: vsphere prepare cloud-config-vsphere-secret.yaml
    template:
      src: cloud-config-vsphere-secret.j2
      dest: /etc/kubernetes/cloud-config-vsphere-secret.yaml
      force: yes
    when:
    - inventory_hostname in groups['primary-master']

  - name: "vpshere apply cloud-config-vsphere-secret.yaml "
    environment: '{{env_kc}}'
    command: kubectl apply -f /etc/kubernetes/cloud-config-vsphere-secret.yaml
    when:
    - inventory_hostname in groups['primary-master']

  - name: "vpshere remove cloud-config-vsphere-secret.yaml "
    file:
      path: /etc/kubernetes/cloud-config-vsphere-secret.yaml
      state: absent
    when:
    - inventory_hostname in groups['primary-master']

  tags:
  - init
  - vpshere_recreate_master_nodes
  when:
  - ClusterConfiguration.cloudProvider is defined 
  - ClusterConfiguration.cloudProvider == "vsphere"

- name: export KUBECONFIG in master's ~/.bash_profile
  lineinfile: 
    dest: ~/.bash_profile
    line: "export KUBECONFIG=/etc/kubernetes/admin.conf"
    state: present
    create: yes
    regexp: '^export KUBECONFIG=.*'
  when: shell is undefined or shell == 'bash'

- name: Wait few seconds for images pulls and cluster services to start
  pause: seconds=3
  changed_when: false

- name: Forcing restart of services
  meta: flush_handlers

- name: Wait few seconds for images pulls and cluster services to start
  pause: seconds=10
  changed_when: false

## In the below 4 cases, we should have used master_name, but k8s does not follow the fqdn definitions...
- name: when cluster is one machine only, remove NoSchedule taint from master - using ansible_fqdn
  ## TODO: Use InitConfiguration to remove the taint on master, with the same condition.
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl taint nodes {{ ansible_fqdn }} {{ item }} --overwrite'
  when:
  - groups['all'] | length == 1
  with_items: #'{{ taints_master }}'
  - 'node-role.kubernetes.io/master:NoSchedule-'
  - 'node-role.kubernetes.io/master=:PreferNoSchedule'
  - 'node-role.kubernetes.io/infra=:PreferNoSchedule'
  ignore_errors: yes
  tags:
  - taints

- name: when cluster is one machine only, remove NoSchedule taint from master - using inventory_hostname_short
  ## TODO: Use InitConfiguration to remove the taint on master, with the same condition.
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl taint nodes {{ inventory_hostname_short }} {{ item }} --overwrite'
  when:
  - groups['all'] | length == 1
  with_items: #'{{ taints_master }}'
  - 'node-role.kubernetes.io/master:NoSchedule-'
  - 'node-role.kubernetes.io/master=:PreferNoSchedule'
  - 'node-role.kubernetes.io/infra=:PreferNoSchedule'
  ignore_errors: yes
  tags:
  - taints

- name: when cluster is one machine only, labeling it also as infra node - using ansible_fqdn
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl label nodes {{ ansible_fqdn }} "node-role.kubernetes.io/infra=" --overwrite'
  when:
  - groups['all'] | length == 1
  register: command_result
  changed_when: '"not labeled" not in command_result.stdout'
  ignore_errors: yes

- name: when cluster is one machine only, labeling it also as infra node - using inventory_hostname_short
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: 'kubectl label nodes {{ inventory_hostname_short }} "node-role.kubernetes.io/infra=" --overwrite'
  when:
  - groups['all'] | length == 1
  register: command_result
  changed_when: '"not labeled" not in command_result.stdout'
  ignore_errors: yes

- name: "sanity - wait for alls pod to be running (besides kube-dns/coredns which won't be ready yet as overlay network is not yet deployed, and workder nodes are not yet installed (on clusters with more than one machine))"
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: "kubectl get --namespace kube-system pods --no-headers | grep -v -w 'Running' | grep -v 'kube-dns' | grep -v 'coredns' || true "
  register: command_result
  tags:
  - sanity
  until: command_result.stdout == ""
  retries: 25
  delay: 5
  changed_when: false

- name: "sanity - make sure master is up (sometimes the above condition is empty as master is in fact not working..."
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  shell: "kubectl get --namespace kube-system pods --no-headers > /dev/null "
  tags:
  - sanity
  delay: 5
  changed_when: false

- name: Set coredns replicas to number of masters (a good practice; by default there are 2 coredns)
  shell: "export KUBECONFIG=/etc/kubernetes/admin.conf; kubectl scale --replicas={{ groups['masters'] | length }} -n kube-system deployment/coredns"
  when:
  - groups['masters'] | length > 1
  - inventory_hostname in groups['primary-master']
  tags:
  - scale
  - scale_dns



