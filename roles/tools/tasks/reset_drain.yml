---
#- hosts: master
#  gather_facts: False
#  become: yes
#  become_method: sudo
 
## TODO:
# 1. We should make sure master is tainted so pods will not move there either.
# 2. k get no emtpy: Now works only if we removed all nodes (which might not be the case; should be limited to machines in groups.node )

- block:
  #- set_fact:
  #    env_kc: '{{ proxy_env |default({}) | combine ({"PATH" : "/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin/"  }) | combine ({"KUBECONFIG" :"/etc/kubernetes/admin.conf"}) }}'
  #  tags:
  #  - always

  - name: get nodes
    shell: "kubectl get nodes --no-headers -- | cut -f 1 -d ' '"
    register: command_results
    changed_when: false

  - name: drain nodes
    command: kubectl drain {{ item }} --delete-local-data --force --ignore-daemonsets --grace-period=5 --timeout=60s
    #with_items: "{{command_results.stdout_lines}}"
    with_items: groups.nodes
    ignore_errors: true

  - name: delete nodes
    command: kubectl delete node {{ item }}
    #with_items: "{{command_results.stdout_lines}}"
    with_items: groups.nodes
    ignore_errors: true

    #shell: "kubectl get nodes -o jsonpath='{.items[*].metadata.name}'"
    #with_items: "{{ groups[cluster_inventory_group.nodes] }}"

  - name: kubectl get nodes must be empty by now (if target was full cluster and not partial update)
    shell: "kubectl get nodes --no-headers | grep -v 'node-role.kubernetes.io/control-plane' | grep -v -w 'Ready' || true"
    register: command_result
    until: command_result.stdout == ""
    retries: 10
    delay: 3
    ignore_errors: true
    changed_when: false

  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: reset_gracefully is defined and reset_gracefully
  tags:
  - reset
  - drain

