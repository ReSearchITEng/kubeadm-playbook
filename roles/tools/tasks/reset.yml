---
#- hosts: all
#  gather_facts: False
#  become: yes
#  become_method: sudo
#  tags:
#  - reset
#  tasks:

- block:

  - name: stop keepalived for cleanup activities
    systemd: name={{ item }} state=stopped
    with_items:
    - keepalived
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes
    when: 
    - groups['masters'] | length > 1 
    - ( custom.networking.masterha_type | default('vip') ) == 'vip'

# We had to remove it, as it blocks the flow. It also fetches docker.io images and in some setups there is no access to or fails due to limits on docker hub...
#  - name: Reset weave network # if it was used
#    shell: /usr/local/bin/weave reset --force
#    ignore_errors: yes
  
  - name: remove pods NFS mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    shell: umount -f $(mount | grep '/kubelet/pods/' | grep '/volumes/kubernetes.io~nfs' | awk '{print $3}')
    tags:
    - umount
    - nfs_reset
    ignore_errors: yes

  - name: Reset cluster (kubeadm reset --force --ignore-preflight-errors=all )
    command: /usr/bin/kubeadm reset --force --ignore-preflight-errors=all
    ignore_errors: yes
    # TODO: if cluster is installed, but kubedm is no longer available on the machine, we will not have a reset of cluster...

  - name: Reset cluster (kubeadm reset --force --ignore-preflight-errors=all using --cri-socket loop )
    command: /usr/bin/kubeadm reset --force --ignore-preflight-errors=all --cri-socket={{ item }}
    ignore_errors: yes
    with_items:
    - /var/run/dockershim.sock
    - /var/run/crio/crio.sock
    - /var/run/containerd/containerd.sock
    - /var/run/cri-dockerd.sock

  ### Cleaning full /etc/kubernetes/ ; Starting k8s 1.12 behaves better, at some point we will remove this step:
  - name: ensure old kubeadm config files were removed
    file: state=absent path={{ item }}
    with_items:
    - /etc/kubernetes/
    #- /etc/kubernetes/kubeadm.conf
    #- /etc/kubernetes/kubeadm-master.config
    #- /etc/kubernetes/kubeadm-master.conf
    #- /etc/kubernetes/cloud-config

#  - name: ensure old /etc/kubernetes/ is removed when full_kube_reinstall is true
#    file: state=absent path={{ item }}
#    with_items:
#    - /etc/kubernetes/
#    #- /var/lib/etcd # there might be cases 
#    when: full_kube_reinstall is defined and full_kube_reinstall

  - name: ensure old /var/lib/etcd/member is removed
    file: state=absent path={{ item }}
    with_items:
    - /var/lib/etcd/member
    when: etcd_clean | default(true)

  - name: systemctl stop kube*.*.slice
    shell: 'for i in $(systemctl list-unit-files --no-legend --no-pager -l | grep --color=never -o kube.*\.slice );do echo $i; systemctl stop $i ; done'
    tags:
    - umount

  - name: Reset cluster (kubeadm reset --force) # starting 1.14
    command: /usr/bin/kubeadm reset --force --ignore-preflight-errors=all
    ignore_errors: yes
    # TODO: if cluster is installed, but kubedm is no longer available on the machine, we will not have a reset of cluster...

  - name: stop kubelet and etcd for cleanup activities
    systemd: name={{ item }} state=stopped
    with_items:
    - kubelet
    - etcd
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  - name: unhold before reinstall packages
    shell: apt-mark unhold {{ item }}
    ignore_errors: yes
    with_items:
    - kubeadm
    - kubelet
    - kubectl
    - kubernetes-cni
    - cri-tools
    when:
    - full_kube_reinstall | default (False)
    - full_kube_apt_unhold | default (False)
    - ansible_os_family == "Debian"
    tags:
    - kubelet
    - uninstall

  - name: Remove before reinstall packages
    package: name={{ item }} state=absent
    with_items:
    - kubeadm
    - kubelet
    - kubectl
    - kubernetes-cni
    when: full_kube_reinstall | default (False) #is defined and full_kube_reinstall
    tags:
    - kubelet
    - uninstall

  - name: remove plugins mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    #shell: 'umount $(mount | grep " on /var/lib/kubelet/plugins/kubernetes.io/" | cut -f1 -d" ")'
    shell: umount -f $(mount | grep '/kubelet/plugins/kubernetes.io/' | awk '{print $3}')
    #shell: 'umount $(mount | grep "/kubelet/plugins/kubernetes.io/" | cut -f1 -d" ")'
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  - name: remove pods mount leftovers; Note you have to collect them from the remote storage (e.g. vsphere datastore) also
    shell: umount -f $(mount | grep '/kubelet/pods/' | grep '/volumes/kubernetes.io~' | awk '{print $3}')
    tags:
    - kubelet
    - uninstall
    ignore_errors: yes

  - name: docker network prune -f
    shell: 'docker network prune -f'

  #https://github.com/kubernetes/kubernetes/issues/39557
  - name: cni0/cbr0 IP alloction issue
    shell: 'rm -rf /var/lib/cni/ /var/lib/kubelet/* /etc/cni/ ; ip link delete cni0; ip link delete cbr0 ; ip link delete flannel.1; ip link delete weave'
    ignore_errors: yes
    tags:
    - uninstall
    
  - name: ipvsadm clear
    shell: 'ipvsadm --clear'
    ignore_errors: yes
    tags:
    - uninstall    

  - name: Reset iptables rules # THIS TASK SHOULD BE REMOVED, is not maintained
    shell: iptables-save | awk '/^[*]/ { print $1 } /^:[A-Z]+ [^-]/ { print $1 " ACCEPT" ; } /COMMIT/ { print $0; }' | iptables-restore
    when: iptables_reset is defined and iptables_reset
    ignore_errors: yes
    tags:
    - uninstall

  #- name: restart kubelet for cleanup activities
  #  systemd: name={{ item }} state=restarted
  #  with_items:
  #  - kubelet
  #  when: ! (full_kube_reinstall is defined and full_kube_reinstall )
  #  tags:
  #  - kubelet
  #  - uninstall
  #  ignore_errors: yes

  - name: Remove /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf if present from HA etcd setup time (in MasterHA)
    file: 
      path: /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
      state: absent

  - name: Remove /etc/sysconfig/kubelet if present
    file: 
      path: /etc/sysconfig/kubelet
      state: absent

  tags:
  - reset
