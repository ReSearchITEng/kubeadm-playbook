## HA
CLUSTER_NAME: demok8s  # used only for defining the clusterConfiguration and joinConfiguration k8s config, as well as the below dnsDomain and masterha_fqdn

## ensure you have the DNS set for wildcard, and pointing all the trafic to master or similar setup
custom:
  networking:
    dnsDomain: "{{ CLUSTER_NAME }}.{{ CORP_DNS_DOMAIN | default ('corp.example.com') }}"  # For MasterHA, if you have dns, put the desired cluster domain here. If no DNS change possible on your side, and you want MasterHA, fix the below 2 values accordinly

    ######
    ## masterha_* params are requried when you have MasterHA (meaning when your inventory has "secondary-masters" section not-empty)
    ## Your setup can either use a LoadBalancer (usually a hw one), 
    ## or use a VIP address which keepalived will manage (move the address from one master to another as needed)

    ## Decide on one of 2 the masterha_types possible:
    # "vip" #Choose VIP and you'll have keepalived instaleld and cofigured for the masterha_ip below (default)
    # "lb" #Choose lb when you have a MasterHA LB which load-balances across all your masters, on api port (default 6443) 
    #      # Make sure your LB is setup to forward requests to a specific master ONLY when api port /healthz on that host returns status 200 
    #masterha_type: "vip" # or "lb"
    masterha_ip: "192.0.0.171" #| default('') }}"  # Important when you have MasterHA;  # IP of either your LB or the VIP to be used.
    ## masterha_fqdn is usually the dns name of masterha_ip above. (We cannot get it automatically in ansible...)
    ## This value is important in order to set apiServerCertSANs in the certs correctly
    masterha_fqdn: "master-{{ CLUSTER_NAME }}.{{ CORP_DNS_DOMAIN | default ('corp.example.com') }}"  # Important when you have MasterHA, in order to set apiServerCertSANs correctly
    #masterha_fqdn: "{{ lookup('dig', masterha_ip, 'qtype=PTR') }}" # but requires some pip modules on host...

    #masterha_bindPort: 6443 #default is 6443; We recommend to keep it 6443.
    ### end of masterha topic

    ## When masterha_type is set to "vip", keepalived is deployed automatically. Options to deploy it via linux package (rpm/deb) or using a docker image.
    ## if you move from one type to another, please make sure you manually remove the previous setup 
    ## E.g. moving from package to docker, manually do: systemctl stop keepliaved; systemctl disable keepalived
    ## E.g. moving from docker to package, manually do: docker rm -f keepalived
    ## Can be either 'docker' or 'package' or 'provided' (when already installed outside of this playbook; this playbook will generate the configuration and check script)
    masterha_vip_keepalived_deploy_type: docker
    masterha_vip_keepalived_docker_image: osixia/keepalived:2.0.17 # 2.0.17+; older version do not have curl

    ## The right way is to always define machines with FQDN in the inventory file (hosts file)
    ## using http proxy for getting to internet (outside of the env) works fine, only by setting nodes with fqdn in inventory, without the below settings
    ## Use the below fqdn functionality only if really required (stong understanding of risks)!
    ## Also do not mix (some fqdn some short name).
    fqdn: # decide where to force use fqdn for non masterha and nodes. When set to false, it will use the name as defined in the inventory file
      always: false # makes all the below true
      master: true  # when true, actions like wait/join will be done against dns name instead of IP
      node: false   # when true, the join command will have --node-name set to fqdn. When false, k8s will set based on how node machine answers to the hostname command

# Name of CNI pods to wait for during deployment. defualt is calico-node, but might be eg antrea-agent or others if you replace your CNI
POD_CNI_NAME: "calico-node"

############## THE BELOW SECTION IS NO LONGER RELEVANT, as NETWORK comes via HELM CHARTS (e.g. tigera-operator from calico)
#Define network for K8S services 
SERVICE_NETWORK_CIDR: 10.96.0.0/12

## Select pod Network. One may add mode simply by adding the deployment url and pod netwrod cidr it needs
## This section is obsolete. By default calico is installed using its helm charts (see addons.yaml)
#podNetwork: 'calico'
#'flannel'
#'weavenet'
#'calico'

# flannel:
#   - podSubnet: 10.244.0.0/16
#   - urls:
#     - https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# 
# calico:
#   - podSubnet: 192.168.0.0/16
#   - urls:
#     - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/etcd.yaml
#     - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/rbac.yaml
#     - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/calico.yaml
# 
# weavenet:
#   - podSubnet: 10.32.0.0/12
#   - urls:
#     #- "https://cloud.weave.works/k8s/net?k8s-version={{ClusterConfiguration.kubernetesVersion}}&env.IPALLOC_RANGE={{POD_NETWORK_CIDR | default ('10.32.0.0/12') }}"
#     - "https://cloud.weave.works/k8s/net?k8s-version={{ClusterConfiguration.kubernetesVersion}}&env.IPALLOC_RANGE='10.32.0.0/12'"

POD_NETWORK_CIDR: 10.244.0.0/16 # Exactly this one is required when Flannel network is used. It can be used also for calico which autodetects the range.
#POD_NETWORK_CIDR: '192.168.0.0/16' # Calico is able to autodetect, this should never be required.
#POD_NETWORK_CIDR: '10.32.0.0/12' # Exactly this one is required when Weave network is used (with defaults). If you other network solutions, this entry can be commented out.

#####
## NETWORK
## usually, it's not possible to have more than one network solution (but projects like "Multus" exist)
## options: https://kubernetes.io/docs/admin/addons/
## Usually choices are: flannel, weavenet, calico

## We have moved the networking deploy part of the helm charts (addons.yaml)
## Should you want to use an overlay network that does not have helm chart, uncomment k8s_network_addons_urls along with of its options:

#k8s_network_addons_urls:

## CALICO # For Calico one has to also ensure above setting ClusterConfiguration.networking.podSubnet is set to 192.168.0.0/16 )
## new 2020: as per: https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less
## "If you are using a different pod CIDR with kubeadm, no changes are required - Calico will automatically detect the CIDR based on the running configuration."
  #- https://docs.projectcalico.org/manifests/calico.yaml
#  - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/etcd.yaml
#  - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/rbac.yaml
#  - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/calico.yaml
## Other Calico version (newer, reusing etcd of k8s, but with other limitations, use with care): 
#   - https://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
## OLDER_CALICO:
#   - https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml # versions are 2.4,2.5,2.6


## OR 

## Flanned: (for Flanned one has to also ensure above setting ClusterConfiguration.networking.podSubnet is set to 10.244.0.0/16 )
  ##- https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
  #- https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml # For latest, replace v0.9.0 with master
  #- https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
  # flannel for 1.12 (fixes toleartions, and fix is not in v0.10.0)
  #- https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# OR 

## Weave: #https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ 
 #- https://cloud.weave.works/k8s/net?k8s-version={{ClusterConfiguration.kubernetesVersion}}&{{POD_NETWORK_CIDR | default ('env.IPALLOC_RANGE=10.32.0.0/12') }}
 #- "https://cloud.weave.works/k8s/net?k8s-version={{ClusterConfiguration.kubernetesVersion}}&env.IPALLOC_RANGE={{POD_NETWORK_CIDR | default ('10.32.0.0/12') }}"

# OR
## kube-router
 #- https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml
#####

