#####
# ADDONS which are not helm charts and not related to helm charts# uncomment/ add the desired ones.
# These are run by post_deploy (right after cluster deploy), before the helm role starts.
# k8s_addons_urls:
# - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/node-problem-detector/npd.yaml # rbac ready
#####

#####
# ADDONS related to helm, and called from the helm role (and after the k8s_addons_urls)
# The place to apply crds, webhooks, rbac, etc
pre_helm_manifests: []
## cert-manager crds:
#- https://raw.githubusercontent.com/jetstack/cert-manager/release-0.14/deploy/manifests/00-crds.yaml
#- https://github.com/jetstack/cert-manager/releases/download/v1.11.2/cert-manager.crds.yaml
# MetalLB Operator
#- https://raw.githubusercontent.com/metallb/metallb-operator/main/bin/metallb-operator.yaml

###################
## HELM & CHARTS ##
###################
helm:
  helm_version: v3.18.6 #comment it out to skip installalling it # or "latest" #https://github.com/kubernetes/helm/releases
  #install_script_url: 'https://github.com/kubernetes/helm/raw/master/scripts/get-helm-3' # OBSOLETE
  archive_url: 'https://get.helm.sh' # it expects there the archive: /helm-{{ helm.helm_version }}-linux-{{ HOST_ARCH }}.tar.gz
  repos: ## stable repo is installed by helm by default, no need for its entry here, add only new ones
  #- { name: stable, url: 'http://kubernetes-charts.storage.googleapis.com' }  # for metrics server till this is fixed: https://github.com/kubernetes-sigs/metrics-server/pull/606 and kube-state-metrics (see https://github.com/kubernetes/kube-state-metrics/pull/1237 )
  #- { name: stable, url: 'https://charts.helm.sh/stable' }  # for metrics server till this is fixed: https://github.com/kubernetes-sigs/metrics-server/pull/606 and kube-state-metrics (see https://github.com/kubernetes/kube-state-metrics/pull/1237 )
  - { name: grafana, url: 'https://grafana.github.io/helm-charts' }  # for grafana/grafana which is called by kube-prometehus-stack  #https://github.com/grafana/helm-charts/tree/main/charts/grafana
  - { name: prometheus-community, url: 'https://prometheus-community.github.io/helm-charts' } # for prometheus-community/kube-prometheus-stack and prometheus-community/prometheus-node-exporter # calls grafana/grafana and stable/kube-state-metrics
  - { name: ingress-nginx, url: 'https://kubernetes.github.io/ingress-nginx' }
  #- { name: kured, url: 'https://kubereboot.github.io/kured' } #OLD
  - { name: kubereboot, url: 'https://kubereboot.github.io/charts' }
  #- { name: incubator, url: 'https://charts.helm.sh/incubator' }
  - { name: jetstack, url: 'https://charts.jetstack.io' } # cert-manager
#  - { name: funkypenguin, url: 'https://funkypenguin.github.io/helm-kubernetes-dashboard' } #dashboard 2.0 (till PR kubernetes/dashboard#4502 merged in official repo)
  - { name: kubernetes-dashboard, url: 'https://kubernetes.github.io/dashboard/' } # https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard
    #- { name: flexkube, url: 'https://flexkube.github.io/charts' } # calico helm chart till official will be ready: https://github.com/projectcalico/calico/issues/717 and https://github.com/projectcalico/calico/tree/master/_includes/charts/calico # plan to move to tigera-opearator
  - { name: projectcalico, url: 'https://docs.projectcalico.org/charts' }
  - { name: rook-release, url: 'https://charts.rook.io/release' }
  - { name: nfs-ganesha-server-and-external-provisioner, url: 'https://kubernetes-sigs.github.io/nfs-ganesha-server-and-external-provisioner/' }
  - { name: minio, url: 'https://charts.min.io/' }
  - { name: runix, url: 'https://helm.runix.net' }
  - { name: crossplane-stable, url: 'https://charts.crossplane.io/stable' }
  - { name: fairwinds-stable, url: 'https://charts.fairwinds.com/stable' } #rbac-manager
  - { name: kong, url: 'https://charts.konghq.com/' }
  packages_list: # when not defined, namespace defaults to "default" namespace
  # use "--wait" in the options section if you want to wait till min. pods are up.
  ### List helm charts you wish pre-installed every time cluster is deployed:

###########################
## Networking: calico or other options
## Calico overlay network #
###########################
    #- { name: tigera-operator, repo: projectcalico/tigera-operator, namespace: "", options: '--set tigeraOperator.registry={{ images_repo | default ("quay.io") }} --set calicoctl.image={{ images_repo | default ("quay.io") }}/calico/ctl' }
    #- { name: tigera-operator, repo: projectcalico/tigera-operator, namespace: "", options: '--set installation.registry={{ images_repo | default ("docker.io") }} --set tigeraOperator.registry={{ images_repo | default ("quay.io") }} --set calicoctl.image={{ images_repo | default ("quay.io") }}/calico/ctl --set typha.image={{ images_repo | default ("quay.io") }}/calico/typha --set cni.image={{ images_repo | default ("quay.io") }}/calico/cni --set node.image={{ images_repo | default ("quay.io") }}/calico/node --set flexvol.image={{ images_repo | default ("quay.io") }}/calico/pod2daemon-flexvol --set kubeControllers.image={{ images_repo | default ("quay.io") }}/calico/kube-controllers' }
    - { name: calico, repo: projectcalico/tigera-operator, namespace: "tigera-operator", options: '--set installation.registry={{ images_repo | default ("docker.io") }} --set tigeraOperator.registry={{ images_repo | default ("quay.io") }} --set calicoctl.image={{ images_repo | default ("quay.io") }}/calico/ctl --set typha.image={{ images_repo | default ("quay.io") }}/calico/typha --set cni.image={{ images_repo | default ("quay.io") }}/calico/cni --set node.image={{ images_repo | default ("quay.io") }}/calico/node --set flexvol.image={{ images_repo | default ("quay.io") }}/calico/pod2daemon-flexvol --set kubeControllers.image={{ images_repo | default ("quay.io") }}/calico/kube-controllers' }
      #- { name: calico, repo: flexkube/calico, namespace: kube-system, options: '--set podCIDR="" --set typha.image.registry={{ images_repo | default ("docker.io") }} --set node.image.registry={{ images_repo | default ("docker.io") }}' } # if autodetect does not work, use '--set podCIDR={{ POD_NETWORK_CIDR }}' } # if not needed, also add --skip-crds

########################
## Monitoring: prometheus, using coreos's prometheus-operator, which includes: grafana, alertmanager, prometheus.
## PROMETHEUS Operator #
########################
# k delete crd alertmanagers.monitoring.coreos.com prometheusrules.monitoring.coreos.com servicemonitors.monitoring.coreos.com prometheuses.monitoring.coreos.com podmonitors.monitoring.coreos.com
# helm install --name prometheus stable/prometheus-operator --namespace monitoring
    - { name: prometheus, repo: prometheus-community/kube-prometheus-stack, namespace: monitoring, options: ' --timeout=15m --set prometheusOperator.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheusOperator.tolerations[0].effect=NoSchedule,prometheusOperator.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheusOperator.tolerations[1].effect=PreferNoSchedule,prometheusOperator.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus.prometheusSpec.tolerations[0].effect=NoSchedule,prometheus.prometheusSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.tolerations[1].effect=PreferNoSchedule,prometheus.prometheusSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.maximumStartupDurationSeconds=300 --set alertmanager.alertmanagerSpec.nodeSelector."node\-role\.kubernetes\.io/infra"= --set alertmanager.alertmanagerSpec.tolerations[0].effect=NoSchedule,alertmanager.alertmanagerSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.tolerations[1].effect=PreferNoSchedule,alertmanager.alertmanagerSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set grafana.nodeSelector."node\-role\.kubernetes\.io/infra"= --set grafana.tolerations[0].effect=NoSchedule,grafana.tolerations[0].key="node-role.kubernetes.io/infra" --set grafana.tolerations[1].effect=PreferNoSchedule,grafana.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.ingress.enabled=True --set prometheus.ingress.pathType=Prefix --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.pathType=Prefix --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} --set prometheusOperator.image.registry={{ images_repo | default ("quay.io") }} --set prometheusOperator.configmapReload.image.registry={{ images_repo | default ("docker.io") }} --set prometheusOperator.prometheusConfigReloader.image.registry={{ images_repo | default ("quay.io") }} --set prometheusOperator.admissionWebhooks.patch.image.registry={{ images_repo | default ("registry.k8s.io") }} --set prometheus.prometheusSpec.image.registry={{ images_repo | default ("quay.io") }} --set alertmanager.alertmanagerSpec.image.registry={{ images_repo | default ("quay.io") }} --set alertmanager.configmapReload.image.registry={{ images_repo | default ("docker.io") }} --set grafana.image.registry={{ images_repo | default ("docker.io") }} --set kube-state-metrics.image.registry={{ images_repo | default ("registry.k8s.io") }} --set kube-state-metrics.tolerations[0].effect=NoSchedule,kube-state-metrics.tolerations[0].key="node-role.kubernetes.io/infra" --set kube-state-metrics.tolerations[1].effect=PreferNoSchedule,kube-state-metrics.tolerations[1].key="node-role.kubernetes.io/infra" --set kube-state-metrics.nodeSelector."node\-role\.kubernetes\.io/infra"= --set prometheus-node-exporter.image.registry={{ images_repo | default ("quay.io") }} --set grafana.sidecar.image.registry={{ images_repo | default ("quay.io") }} --set grafana.imageRenderer.image.registry={{ images_repo | default ("docker.io") }} --set prometheusOperator.admissionWebhooks.enabled=false --set prometheusOperator.tls.enabled=false --set prometheus-node-exporter.hostRootFsMount.enable=false ' }   # --set prometheus-node-exporter.hostRootFsMount=false # --set nodeExporter.hostRootfs=false --set nodeExporter.hostRootFsMount=false

    #--set prometheus.ingress.enabled=True --set alertmanager.ingress.enabled=True # TEMP DISABLED DUE TO A BUG: is invalid: spec.rules[0].http.paths[0].pathType: Required value: pathType must be specified
    # - { name: prometheus, repo: stable/prometheus-operator, namespace: monitoring, options: ' --timeout=15m --set prometheusOperator.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheusOperator.tolerations[0].effect=NoSchedule,prometheusOperator.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheusOperator.tolerations[1].effect=PreferNoSchedule,prometheusOperator.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus.prometheusSpec.tolerations[0].effect=NoSchedule,prometheus.prometheusSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set prometheus.prometheusSpec.tolerations[1].effect=PreferNoSchedule,prometheus.prometheusSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.nodeSelector."node\-role\.kubernetes\.io/infra=" --set alertmanager.alertmanagerSpec.tolerations[0].effect=NoSchedule,alertmanager.alertmanagerSpec.tolerations[0].key="node-role.kubernetes.io/infra" --set alertmanager.alertmanagerSpec.tolerations[1].effect=PreferNoSchedule,alertmanager.alertmanagerSpec.tolerations[1].key="node-role.kubernetes.io/infra" --set grafana.nodeSelector."node\-role\.kubernetes\.io/infra=" --set grafana.tolerations[0].effect=NoSchedule,grafana.tolerations[0].key="node-role.kubernetes.io/infra" --set grafana.tolerations[1].effect=PreferNoSchedule,grafana.tolerations[1].key="node-role.kubernetes.io/infra" --set prometheus.ingress.enabled=True --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} --set prometheusOperator.image.repository={{ images_repo | default ("quay.io") }}/coreos/prometheus-operator --set prometheusOperator.configmapReloadImage.repository={{ images_repo | default ("docker.io") }}/jimmidyson/configmap-reload --set prometheusOperator.prometheusConfigReloaderImage.repository={{ images_repo | default ("quay.io") }}/coreos/prometheus-config-reloader --set prometheusOperator.admissionWebhooks.patch.image={{ images_repo | default ("registry.k8s.io") }}/ingress-nginx/kube-webhook-certgen --set prometheus.prometheusSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/prometheus --set alertmanager.alertmanagerSpec.image.repository={{ images_repo | default ("quay.io") }}/prometheus/alertmanager --set prometheusOperator.hyperkubeImage.repository={{ images_repo | default ("registry.k8s.io") }}/hyperkube --set grafana.image.repository={{ images_repo | default ("docker.io") }}/grafana/grafana --set kube-state-metrics.image.repository={{ images_repo | default ("quay.io") }}/coreos/kube-state-metrics --set kube-state-metrics.tolerations[0].effect=NoSchedule,kube-state-metrics.tolerations[0].key="node-role.kubernetes.io/infra" --set kube-state-metrics.tolerations[1].effect=PreferNoSchedule,kube-state-metrics.tolerations[1].key="node-role.kubernetes.io/infra" --set kube-state-metrics.nodeSelector."node\-role\.kubernetes\.io/infra=" --set prometheus-node-exporter.image.repository={{ images_repo | default ("quay.io") }}/prometheus/node-exporter ' }  # --set prometheusOperator.admissionWebhooks.enabled=false

    #- { name: prometheus, repo: stable/prometheus-operator, namespace: monitoring, options: '--set prometheus.ingress.enabled=True --set prometheus.ingress.hosts[0]=prometheus.{{ custom.networking.dnsDomain }} --set grafana.ingress.enabled=True --set grafana.ingress.hosts[0]=grafana.{{ custom.networking.dnsDomain }} --set alertmanager.ingress.enabled=True --set alertmanager.ingress.hosts[0]=alertmanager.{{ custom.networking.dnsDomain }} ' }

################
#### Heapster ##
################
# Will be deprecated as soon as we can use metrics-server from dashboard 2.0 helm chart will be officially released.
#    - { name: heapster, repo: stable/heapster, namespace: kube-system, options: '--set service.nameOverride=heapster,rbac.create=true --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set image.repository={{ images_repo | default ("registry.k8s.io") }}/heapster-{{ HOST_ARCH }} --set resizer.image.repository={{ images_repo | default ("registry.k8s.io") }}/addon-resizer --set resizer.enabled=False ' }
# --set resizer.enabled=False ->> this is not working with taints due to some bug, thefore disabling it for now.
#    - { name: heapster, repo: stable/heapster, namespace: kube-system, options: '--set service.nameOverride=heapster,rbac.create=true' }

#####################
## Metrics-Server ###
#####################

    #IF p8s is not installed, install this manually:
    #- { name: metrics-server, repo: prometheus-community/kube-state-metrics, namespace: monitoring, options: '--set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set image.repository={{ images_repo | default ("registry.k8s.io") }}/kube-state-metrics/kube-state-metrics --set "args={--kubelet-preferred-address-types=InternalIP}" ' }
      #- { name: metrics-server, repo: prometheus-community/metrics-server, namespace: monitoring, options: '--set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set image.repository={{ images_repo | default ("registry.k8s.io") }}/metrics-server-{{ HOST_ARCH }} --set "args={--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}" ' }
# Proper fix for "--kubelet-insecure-tls" will be in k8s 1.19 https://github.com/kubernetes/kubeadm/issues/1602 ; Alternatives is manually generating and approving certs for each node: serverTLSBootstrap, which is complex. cert-manager option was not investigated.: https://github.com/kubernetes-sigs/metrics-server/issues/146#issuecomment-472655656

##################
## cert-manager ##
##################
    - { name: cert-manager, repo: jetstack/cert-manager, namespace: cert-manager, options: '--set installCRDs=true --set prometheus.servicemonitor.enabled=true --set prometheus.servicemonitor.namespace=monitoring --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-controller --set webhook.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-webhook --set cainjector.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-cainjector --set startupapicheck.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-ctl --set acmesolver.image.repository={{ images_repo | default ("quay.io") }}/jetstack/cert-manager-acmesolver --set http_proxy={{proxy_env.http_proxy | default ("") }},https_proxy={{proxy_env.https_proxy | default ("") }},no_proxy={{proxy_env.no_proxy | default ("") | replace(",","\\,") }} --version ~1.14' }
      #--set prometheus.servicemonitor.labels=prometheusoperator

#####################
## RBAC-MANAGER #######
#####################
    - { name: rbac-manager, repo: fairwinds-stable/rbac-manager, namespace: "rbac-manager", options: '--set metrics.enabled=true --set image.repository={{ images_repo | default ("quay.io") }}/reactiveops/rbac-manager'}

#####################
## CROSSPLANE #######
#####################
    - { name: crossplane, repo: crossplane-stable/crossplane, namespace: "crossplane-system", options: '--set metrics.enabled=true --set image.repository={{ images_repo | default ("xpkg.upbound.io") }}/crossplane/crossplane '}

################
## DASHBOARD ###
################
## This (v1) will be deprecated in favour of 2.0 - soon to be released
#    - { name: dashboard, repo: stable/kubernetes-dashboard, namespace: kube-system, options: '--set image.repository={{ images_repo | default ("registry.k8s.io") }}/kubernetes-dashboard-{{ HOST_ARCH }} --set rbac.create=True,ingress.enabled=True,ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},ingress.hosts[1]={{ custom.networking.masterha_fqdn | default (groups["primary-master"][0]) }},ingress.hosts[2]={{ groups["primary-master"][0] }} --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set rbac.create=True,rbac.clusterAdminRole=True --set enableInsecureLogin=True --set enableSkipLogin=True ' }
# For a learning/development --set rbac.clusterAdminRole=True with skip login and insecure might be acceptable, but not for real case scenarios!!!
# For in between, one can keep: rbac.clusterReadOnlyRole=True (if bug https://github.com/helm/charts/issues/15118 was solved)
# For a production, remove --set enableInsecureLogin=True --set enableSkipLogin=True --set rbac.clusterAdminRole=True

# Option 2:
# use the below if you are sure you don't need any auth to your dashboard, and you use k8s 1.15 or older.
#    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["primary-master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},image.tag=v1.8.3 --version=0.5.3' }

####################
## DASHBOARD 2.0 ###
####################
    - { name: dashboard, repo: kubernetes-dashboard/kubernetes-dashboard, namespace: monitoring, options: '--set image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard --set ingress.enabled=True,ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},ingress.hosts[1]={{ custom.networking.masterha_fqdn | default (groups["primary-master"][0]) }},ingress.hosts[2]={{ groups["primary-master"][0] }} --set nodeSelector."node\-role\.kubernetes\.io/infra=" --set tolerations[0].effect=NoSchedule,tolerations[0].key="node-role.kubernetes.io/infra" --set tolerations[1].effect=PreferNoSchedule,tolerations[1].key="node-role.kubernetes.io/infra" --set metricsScraper.enabled=true,metricsScraper.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard-metrics-scraper --set rbac.create=True,rbac.clusterReadOnlyRole=True --set protocolHttp=true --set kong.image.repository={{ images_repo | default ("docker.io") }}/kong --set kong.admin.tls.enabled=false --set api.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard-api --set web.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard-web --set auth.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/dashboard-auth --set api.scaling.replicas=1 --set app.ingress.enabled=True,app.ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},app.ingress.hosts[1]={{ custom.networking.masterha_fqdn | default (groups["primary-master"][0]) }},app.ingress.hosts[2]={{ groups["primary-master"][0] }} --set app.scheduling.nodeSelector."node\-role\.kubernetes\.io/infra=" --set app.tolerations[0].effect=NoSchedule,app.tolerations[0].key="node-role.kubernetes.io/infra" --set app.tolerations[1].effect=PreferNoSchedule,app.tolerations[1].key="node-role.kubernetes.io/infra" --version ^6 --set metricsScraper.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/metrics-scraper --set rbac.create=True,rbac.clusterReadOnlyRole=True --set protocolHttp=true '}
      
      
      #metricsScraper.image.repository={{ images_repo | default ("docker.io") }}/kubernetesui/metrics-scraper --set rbac.create=True,rbac.clusterReadOnlyRole=True --set protocolHttp=true' } # --version 4.0.0' } # https://github.com/kubernetes/dashboard/blob/master/aio/deploy/helm-chart/kubernetes-dashboard/Chart.yaml#L17

################
## Kured #######
## For restaring nodes when needed, in sequencial and proper manner
################
    - { name: kured, repo: kubereboot/kured, namespace: kube-system, options: '--set extraArgs.period="0h07m0s" --set image.repository={{ images_repo | default ("ghcr.io") }}/kubereboot/kured ' } #--version 1.5.1 https://github.com/weaveworks/kured/tree/master/charts/kured

################
## metallb #####
################
#    - { name: metallb, repo: stable/metallb, namespace: infra, options: '--set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.image.repository={{ images_repo | default ("docker.io") }}/metallb/controller --set speaker.nodeSelector."node\-role\.kubernetes\.io/infra=" --set speaker.image.repository={{ images_repo | default ("docker.io") }}/metallb/speaker --set prometheus.serviceMonitor.enabled=True --set prometheus.prometheusRule.enabled=True' }
    - { name: metallb, repo: "oci://registry-1.docker.io/bitnamicharts/metallb", namespace: infra, options: '--set controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true,controller.metrics.serviceMonitor.enabled=true --set installCRDs=true --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set global.imageRegistry={{ images_repo | default ("docker.io") }} --set speaker.nodeSelector."node\-role\.kubernetes\.io/infra=" --set speaker.metrics.enabled=True,speaker.metrics.serviceMonitor.enabled=True --set prometheusRule.enabled=True' }

####################
## INGRESS NGINX ###
####################
# (the new nginx ingress )
    - { name: ingress-nginx, repo: ingress-nginx/ingress-nginx, namespace: kube-system, options: '--set controller.watchIngressWithoutClass=true --set controller.ingressClassResource.default=true --set controller.hostNetwork=true --set controller.admissionWebhooks.enabled=false --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=false --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=ClusterIP --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.admissionWebhooks.patch.image.registry={{ images_repo | default ("registry.k8s.io") }} --set defaultBackend.image.image=defaultbackend-{{ HOST_ARCH | default ("amd64") }} --set controller.extraArgs.enable-ssl-passthrough="true" ' } #--version 4.0.1' } #https://github.com/kubernetes/ingress-nginx/blob/master/charts/ingress-nginx/Chart.yaml#L5    # PARAMS explained: https://kubernetes.github.io/ingress-nginx/deploy/baremetal/ and https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml

####################
## NGINX INGRESS ###
####################
    ### Option 1 - Daemonset & NodePort, (no prometheus Monitoring)
    #- { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set controller.watchIngressWithoutClass=true --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=false --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("us.gcr.io") }} --set controller.image.repository=k8s-artifacts-prod/ingress-nginx/controller --set defaultBackend.image.repository={{ images_repo | default ("registry.k8s.io") }}/defaultbackend-{{ HOST_ARCH }} --version 1.41.1' }
    ### Option 1b - Daemonset & NodePort like above, but with prometheus Monitoring => make sure prometheus chart is also enabled
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set controller.watchIngressWithoutClass=true --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.kind=DaemonSet --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.admissionWebhooks.patch.image.registry={{ images_repo | default ("registry.k8s.io") }} --set defaultBackend.image.image=defaultbackend-{{ HOST_ARCH | default ("amd64") }} ' }
    # --set controller.service.externalTrafficPolicy="Local" # for better performance, when you have a LB for the ingress controllers (across all nodes with role label infra), add this as well, or use the useHostPort option below.

    ### Option 2 - DaemonSet & hostPort - Use it when there is only one machine
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set controller.watchIngressWithoutClass=true --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set controller.service.type=ClusterIP --set controller.kind=DaemonSet --set controller.daemonset.useHostPort=true --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.admissionWebhooks.patch.image.registry={{ images_repo | default ("registry.k8s.io") }} --set defaultBackend.image.image=defaultbackend-{{ HOST_ARCH | default ("amd64") }} ' }
    # https://rimusz.net/migrating-to-ingress-nginx hostPort setting was moved to: hostPort.enabled=true

    ### Option 3 - Deployment & NodePort
    # - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set controller.watchIngressWithoutClass=true --set rbac.create=true,serviceAccount.create=true --set controller.stats.enabled=true,controller.metrics.enabled=true,controller.metrics.serviceMonitor.enabled=true --set controller.metrics.serviceMonitor.namespace=monitoring --set controller.metrics.serviceMonitor.additionalLabels.monitoring=prometheusoperator --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443  --set controller.tolerations[0].effect=NoSchedule,controller.tolerations[0].key="node-role.kubernetes.io/infra" --set controller.tolerations[1].effect=PreferNoSchedule,controller.tolerations[1].key="node-role.kubernetes.io/infra" --set controller.nodeSelector."node\-role\.kubernetes\.io/infra=" --set-string controller.config.server-tokens=false --set controller.config.hide-headers=Server --set controller.kind=Deployment --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.image.registry={{ images_repo | default ("registry.k8s.io") }} --set controller.admissionWebhooks.patch.image.registry={{ images_repo | default ("registry.k8s.io") }} --set defaultBackend.image.image=defaultbackend-{{ HOST_ARCH | default ("amd64") }} ' }
# --set controller.service.externalTrafficPolicy="Local" # See notes above


####################
## ROOK.IO STORAGE #
####################
    #- { name: rook-ceph, repo: rook-release/rook-ceph, namespace: rook-ceph, options: 'image.repository={{ images_repo | default ("docker.io") }}/rook/ceph --set csi.cephcsi.image={{ images_repo | default ("quay.io") }}/cephcsi/cephcsi:v3.1.2 --set csi.registrar.image={{ images_repo | default ("registry.k8s.io") }}/sig-storage/csi-node-driver-registrar:v2.0.1 --set csi.resizer.image {{ images_repo | default ("registry.k8s.io") }}/sig-storage/csi-resizer:v1.0.0 --set csi.provisioner.image {{ images_repo | default ("registry.k8s.io") }}/sig-storage/csi-provisioner:v2.0.0 --set csi.snapshotter.image {{ images_repo | default ("registry.k8s.io") }}/sig-storage/csi-snapshotter:v3.0.0 --set csi.attacher.image {{ images_repo | default ("registry.k8s.io") }}/sig-storage/csi-attacher:v3.0.0 ' }
#####
#

    #- { name: nfs-server-provisioner, repo: nfs-ganesha-server-and-external-provisioner/nfs-server-provisioner, namespace: kube-system, options: '--set image.repository="{{ images_repo | default ("registry.k8s.io") }}/sig-storage/nfs-provisioner",persistence.enabled=true,persistence.size=500Gi,persistence.storageClass="base4nfs-ganesha-server-and-external-provisioner",storageClass.defaultClass="true",nodeSelector."kubernetes\\.io/hostname"={{ inventory_hostname_short }} --set storageClass.mountOptions[0]="timeo=600" ' }

##############################
## MIN.IO S3 Object STORAGE ##
##############################
    #- { name: minio, repo: minio/minio, namespace: minio, options: '--set rootUser=rootuser,rootPassword=rootpass123 --set resources.requests.memory=512Mi --set mode=standalone --set image.repository={{ images_repo | default ("quay.io") }}/minio/minio --set mcImage.repository={{ images_repo | default ("quay.io") }}/minio/mc --set ingress.enabled=true,ingress.hosts[0]=minio.{{ custom.networking.dnsDomain }} --set consoleIngress.enabled=true,consoleIngress.hosts[0]=console-minio.{{ custom.networking.dnsDomain }} ' }
#####
#
#

##############################
## PGAdmin4 Postgress UI    ##
##############################
    #- { name: pgadmin4, repo: runix/pgadmin4, namespace: pg, options: '--set persistentVolume.size="1Gi" --set image.registry={{ images_repo | default ("docker.io") }}' }
