---
## Preparations
## Making sure python exists on all nodes, so Ansible will be able to run:
- hosts: all
  gather_facts: False
  become: yes
  become_method: sudo
  pre_tasks:
  - name: Install python2 for Ansible (usually required on ubuntu, which defaults to python3) # Alternativelly, for Ubuntu machines, define var: ansible_python_interpreter=/usr/bin/python3
    raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) || (yum install -y python2 python-simplejson)
    register: output
    changed_when: output.stdout != ""
    tags: always

  - setup: # aka gather_facts
    tags: always # required for tags, see ansible issue: #14228
    
  - name: test min. vars (group_vars/all) are set (kubeadm_master_config and k8s_network_addons_urls)
    debug: msg='Make sure min. vars are set in group_vars/all (e.g. kubeadm_master_config and k8s_network_addons_urls)'
    when: kubeadm_master_config is not defined
    failed_when: kubeadm_master_config is not defined
    tags: always # always check if we have vars in place

## proper reset of any previous cluster (if any)
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - reset
  - master
  roles:
  - { role: helm, task: helm_reset, tags: [ 'reset', 'helm_reset' ] }
  - { role: storage, task: remove_pvs, tags: [ 'reset', 'storage_reset', 'pvs_reset' ] }
  - { role: storage, task: nfs_reset, tags: [ 'reset', 'storage_reset', 'nfs_reset' ] }
  - { role: storage, task: rook_reset, tags: [ 'reset', 'storage_reset', 'rook_reset' ] }
  - { role: tools, task: reset_drain, tags: [ 'reset', 'node_reset', 'drain', 'node_drain' ] } #done on master, affecting nodes

## nodes -> reset and install common part (for all nodes)
- hosts: node
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - node
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'node_reset' ], when: "inventory_hostname not in groups['master']" }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'node_reset', 'network_reset', 'weave_reset', 'weave' ], when: "inventory_hostname not in groups['master']" }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'node_install', 'node' ], when: "inventory_hostname not in groups['master']" }

## etcd -> reset and install common part (for all etcds, when not in the same group with master)
- hosts: etcd
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - etcd
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'etcd_reset' ], when: "inventory_hostname not in groups['master']" }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'etcd_reset', 'network_reset', 'weave_reset', 'weave' ], when: "inventory_hostname not in groups['master']" }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'etcd_install', 'node' ], when: "inventory_hostname not in groups['master']" }
    
## master -> reset and install common part (for all masters - and sometimes etcd when colocated with masters)
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - master
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'master_reset' ] }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'master_reset', 'network_reset', 'weave', 'weave_reset' ] }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'master_install'] }

## etcd -> install etcd (when HA)
- hosts: etcd
  gather_facts: all # smart
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - etcd
  - install
  - etcd_install
  - ha
  roles:
  - { role: etcd, tags: [ 'install', 'etcd_install', 'ha', 'etcd'], when: "groups['master'] | length > 1" } 

## master -> install keepalived and deploy etcd client certs on masters (relevat if HA)
- hosts: master
  gather_facts: all # smart
  become: yes
  become_method: sudo
  any_errors_fatal: yes
  tags:
  - master
  - install
  - ha
  roles:
  - { role: keepalived, tags: [ 'master', 'install', 'master_install', 'ha', 'keepalived'], when: "groups['master'] | length > 1" }
  - { role: client-certs, tags: [ 'master', 'install', 'master_install', 'ha', 'client-certs'], when: "groups['master'] | length > 1" }

- hosts: primary-master 
  name: (or master in general; applies to both ha and non-ha)
  gather_facts: all # smart
  become: yes
  become_method: sudo
  tags:
  - master
  - install
  roles:
  - { role: master, tags: [ 'primary-master', 'master', 'install', 'master_install'] } 

- hosts: secondary-masters
  gather_facts: all # smart
  become: yes
  become_method: sudo
  tags:
  - master
  - install
  - ha
  roles:
  - { role: master, tags: [ 'secondary-masters', 'master', 'install', 'master_install', 'secondary_masters'] } 

## node -> install nodes (kubeadm join, etc)
- hosts: node
  gather_facts: smart
  become: yes
  become_method: sudo
  roles:
  - { role: node, tags: [ 'node', 'install', 'node_install'], when: "inventory_hostname not in groups['master']" }

## Post deploy (network, storage, helm installation, helm charts deploy, any other addons)
- hosts: primary-master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - post_deploy
  roles:
  - { role: post_deploy, task: all, tags: [ 'post_deploy' ] }
  - { role: storage, task: create_all, tags: [ 'storage', 'rook', 'nfs', 'vsphere' ] }
  - { role: helm, task: all, tags: [ 'helm', 'post_deploy' ] }

## Generic Sanity
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - sanity
  - node_install
  - node
  - master
  roles:
  - { role: tools, task: cluster_sanity, tags: [ 'cluster_sanity', 'sanity' ] }
  - { role: tools, task: postinstall_messages, tags: [ 'cluster_sanity', 'sanity' ] }

## to reset/add only some (more) nodes:
##   1. keep in hosts only:
##      - the master
##      - the affected node (all other nodes should not be there)
##   2. Have the token defined in the group_vars/all
##   3. Run using only this/these tag(s):
## ansible-playbook -i hosts -v site.yml --tags "node"   # same with: ansible-playbook -i hosts -v site.yml --tags "node_reset,node_install,cluster_sanity,cluster_info"

## To get cluster info/sanity:
## ansible-playbook -i hosts -v site.yml --tags "cluster_sanity,cluster_info"
