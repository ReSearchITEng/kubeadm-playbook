---
## Preparations
## Making sure python exists on all nodes, so Ansible will be able to run:
- hosts: all
  gather_facts: False
  become: yes
  become_method: sudo
  pre_tasks:
  - name: Install python2 for Ansible (usually required on ubuntu, which defaults to python3) # Alternativelly, for Ubuntu machines, define var: ansible_python_interpreter=/usr/bin/python3
    raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal) || (yum install -y python2 python-simplejson)
    register: output
    changed_when: output.stdout != ""
    tags: always

  - setup: # aka gather_facts
    tags: always # required for tags, see ansible issue: #14228
    
  - name: test min. vars (group_vars/all) are set (kubeadm_master_config and k8s_network_addons_urls)
    debug: msg='Make sure min. vars are set in group_vars/all (e.g. kubeadm_master_config and k8s_network_addons_urls)'
    when: kubeadm_master_config is not defined
    failed_when: kubeadm_master_config is not defined
    tags: always # always check if we have vars in place

## proper reset of any previous cluster (if any)
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - reset
  - master
  roles:
  - { role: helm, task: helm_reset, tags: [ 'reset', 'helm_reset' ] }
  - { role: storage, task: remove_pvs, tags: [ 'reset', 'storage_reset', 'pvs_reset' ] }
  - { role: storage, task: nfs_reset, tags: [ 'reset', 'storage_reset', 'nfs_reset' ] }
  - { role: storage, task: rook_reset, tags: [ 'reset', 'storage_reset', 'rook_reset' ] }
  - { role: tools, task: reset_drain, tags: [ 'reset', 'node_reset', 'drain', 'node_drain' ] } #done on master, affecting nodes

## nodes -> reset and install common part (for all machines)
- hosts: node
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - node
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'node_reset' ] }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'node_reset', 'network_reset', 'weave', 'weave_reset' ] }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'node', 'node_install'] }

## master -> reset and install common part (for all machines)
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - master
  roles:
  - { role: tools, task: reset, tags: [ 'reset', 'master_reset' ] }
  - { role: tools, task: weave_reset, tags: [ 'reset', 'master_reset', 'network_reset', 'weave', 'weave_reset' ] }
  - { role: common, task: all, tags: [ 'common', 'install', 'common_install', 'master_install'] }

## master -> install master (kubeadm init, etc)
- hosts: master
  gather_facts: all # smart
  become: yes
  become_method: sudo
  tags:
  - master
  - install
  roles:
  - { role: master, tags: [ 'master', 'install', 'master_install'] } #, task: all }

## node -> install nodes (kubeadm join, etc)
- hosts: node
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - node
  - install
  - node_install
  tasks:
  - name: node conditional role
    include_role:
      name: node
    when: inventory_hostname not in groups['master']

## Post deploy (network, storage, helm installation, helm charts deploy, any other addons)
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - post_deploy
  roles:
  - { role: post_deploy, task: all, tags: [ 'post_deploy' ] }
  - { role: storage, task: create_all, tags: [ 'storage', 'rook', 'nfs', 'vsphere' ] }
  - { role: helm, task: all, tags: [ 'helm', 'post_deploy' ] }

## Generic Sanity
- hosts: master
  gather_facts: smart
  become: yes
  become_method: sudo
  tags:
  - sanity
  - node_install
  - node
  - master
  roles:
  - { role: tools, task: cluster_sanity, tags: [ 'cluster_sanity', 'sanity' ] }
  - { role: tools, task: postinstall_messages, tags: [ 'cluster_sanity', 'sanity' ] }

## to reset/add only some (more) nodes:
##   1. keep in hosts only:
##      - the master
##      - the affected node (all other nodes should not be there)
##   2. Have the token defined in the group_vars/all
##   3. Run using only this/these tag(s):
## ansible-playbook -i hosts -v site.yml --tags "node"   # same with: ansible-playbook -i hosts -v site.yml --tags "node_reset,node_install,cluster_sanity,cluster_info"

## To get cluster info/sanity:
## ansible-playbook -i hosts -v site.yml --tags "cluster_sanity,cluster_info"
